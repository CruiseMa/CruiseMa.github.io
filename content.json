[{"title":"第一章：导言","date":"2017-02-17T11:04:58.000Z","path":"2017/02/17/密码学与网络安全/","text":"1.1安全目标 机密性(confidentiality)：避免未经授权的访问 完整性(integrity)：避免未经授权的更改 可用性(availability)：对授权实体随时可用 1.2安全攻击 威胁机密性的攻击 窃听(snooping)：在未经授权的情况下访问或拦截信息。 流量分析(traffic analysis)：虽然通过加密可使文件变成对拦截者不可解的信息，但还可以通过在线监测流量获得其他形式的信息。例如获得发送者或接收者的电子地址。 威胁完整性的攻击 篡改(modification)：拦截或访问信息后，攻击者可以修改信息使其对己有利。 伪装(masquerading)：攻击者假扮成某人。例如，伪装为银行的客户，从而盗取银行客户的银行卡密码和个人身份号码。例如，当用户设法联系某银行的时候，伪装为银行，从用户那里得到某些相关的信息。 重放(replaying)：即攻击者获得用户所发信息的拷贝后再重放这些信息。例如，某人向银行发送一项请求，要求向为他工作过的攻击者支付酬金。攻击者拦截这一信息后，重新发送该信息，就会从银行再得到一笔酬金。 否认(repudiation)：发送者否认曾经发送过信息，或接收者否认曾经接收过信息。 例：客户要求银行向第三方支付一笔钱，但是后来又否认她曾经有过这种要求。 某人购买产品并进行了电子支付，制造商却否认曾经获得过支付并要求重新支付。 威胁可用性的攻击 拒绝服务(denial of service)：可能减缓或完全中断系统的服务。攻击者可以通过几种策略来实现其目的。 发送大量虚假请求，以致服务器由于超负荷而崩溃； 拦截并删除服务器对客户的答复，使客户认为服务器没有做出反应； 从客户方拦截这种请求，造成客户反复多次地发送请求并使系统超负荷。 被动攻击与主动攻击 在被动攻击(passive attack)中，攻击者的目的只是获取信息，这就意味着攻击者不会篡改信息或危害系统。系统可以不中断其正常运行。威胁信息机密性的攻击——窃听和流量分析均属被动攻击。信息的暴露会危害信息的发送者或接收者，但是系统不会受到影响。因此，在信息发送者或者接收者发现机密信息已经泄露之前，要发现这种攻击是困难的。然而，被动攻击可以通过对信息进行加密而避免。 主动攻击(active attack)可能改变信息或危害系统。威胁信息完整性和有效性的攻击就是主动攻击。主动攻击通常易于探测但却难于防范，因为攻击者可以通过多种方法发起攻击。 1.3安全服务 信息机密性：保护信息免于窃听和流量分析。 信息完整性：保护信息免于被恶意方篡改，插入，删除和重放。 身份认证(authentication)：提供发送方或接收方的身份认证。 对等实体身份认证：在有通信连接的时候在建立连接时认证发送方和接收方的身份； 数据源身份认证：在没有通信连接的时候，认证信息的来源。 不可否认性(nonrepudiation)：保护信息免于被信息发送方或接收方否认。 在带有源证据的不可否认性中，如果信息的发送方否认，信息的接收方过后可以检验其身份； 在带有交接证据的不可否认性中，信息的发送者过后可以检验发送给预定接收方的信息。 访问控制(access control)：保护信息免于被未经授权的实体访问。在这里，访问的含义是非常宽泛的，包含对程序的读、写、修改和执行等 1.4安全机制 加密(encipherment)：隐藏或覆盖信息使其具有机密性，有密码术和密写术两种技术。 信息完整性(data integrity)：附加于一个短的键值，该键值是信息本身创建的特殊程序。接收方接收信息和键值，再从接收的信息中创建一个新的键值，并把新创建的键值和原来的进行比较。如果两个键值相同，则说明信息的完整性被保全。 数字签名(digital signature)：发送方对信息进行电子签名，接收方对签名进行电子检验。发送方使用显示其与公钥有联系的私钥，这个公钥是他公开承认的。接收方使用发送方的公钥，证明信息确实是由声称发送过这个信息的人签名的。 身份认证交换(authentication exchange)：两个实体交换信息以相互证明身份。例如，一方实体可以证明他知道一个只有他才知道的秘密。 流量填充(traffic padding)：在数据流中嵌入一些虚假信息，从而阻止对手企图实用流量分析。 路由控制(routing control)：在发送方和接收方之间不断改变有效路由，避免对手在特定的路由上进行偷听。 公证(notarization)：选择一个双方都信赖的第三方控制双方的通信，避免否认。 访问控制(access control)：用各种方法，证明某用户具有访问该信息或系统资源的权利。密码和PIN即是这方面的例子。 1.5加密技术密码术(cryptography)密码术是指通过加密把信息的内容隐蔽起来。 对称密匙加密：发送方加密和接收方解密使用同一个密钥。 非对称密匙加密：有一把公钥和一把私钥，发送方用接收方的公钥对信息进行加密，接收方用自己的私钥进行解密。 散列法：从可变长度信息创建一个固定长度的信息摘要，信息和信息摘要都发送给接受方，保证信息完整性。 密写术(steganography)密写术是指用另外的某种东西把信息本身隐蔽起来。","tags":[{"name":"网络安全","slug":"网络安全","permalink":"http://yoursite.com/tags/网络安全/"}]},{"title":"机器学习（三）：最小二乘法","date":"2017-02-14T09:53:58.000Z","path":"2017/02/14/最小二乘法/","text":"基于Andrew Ng（吴恩达）的机器学习课程 矩阵导数定义一个函数f，从m*n的矩阵到实数的映射，定义f关于矩阵A的导数为： 因此梯度 ∇Af(A) 本身就是一个m*n的矩阵，对应(i,j)的元素就是∂f/∂Aij。 例： A是一个二维矩阵 函数f定义为 我们得到： 引入第二个概念：迹算子（trace operator），记作’tr’。对于一个nn维的矩阵A，A的迹就是矩阵主对角线上的元素的和![](http://a2.qpic.cn/psb?/V11CXqIz0j9b24/idGT0uac0Px98ZU53m8dQ5XmMtFj9u3tsv5Wkfqnsc!/b/dCIAAAAAAAAA&amp;bo=qQBUAKkAVAADCSw!&amp;rf=viewer_4) 最小二乘学习了矩阵导数，我们开始继续在封闭的模型中寻找θ使得J(θ)取得最小值。 先把J重新写入向量矩阵中： X为mn维的矩阵，包含训练集的输入值![](http://a2.qpic.cn/psb?/V11CXqIz0j9b24/NEz6uPJS5w9f54ePpc2Nkoir2b5a0iGVNJnHHiui2Ts!/b/dCUAAAAAAAAA&amp;bo=QCoAP0AqAADCSw!&amp;rf=viewer_4) Y为一维的列向量，值为所有的目标值 m表示样本的数目,n表示特征的数目。（实际上是m*(n+1)，如果算上偏置项θ0） 对于列向量Z，易证 所以有 最后，为了最小化J，我们寻找J(θ)关于θ的导数","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"机器学习（二）：梯度下降法","date":"2017-02-12T07:07:58.000Z","path":"2017/02/12/梯度下降的数学推导/","text":"基于Andrew Ng（吴恩达）的机器学习课程 在可以预期的未来，大数据与人工智能方面的工作机会将会快速增长，聪明的人已经在提前进行布局。要学习这个领域，我个人认为其难点不在于计算机技术本身，而在于相关的数学基础，以及对特定应用领域的了解。数学好，你才能够真正理解和用好各种算法，并进而掌握各种计算机工具、框架和平台，具备对数据进行各种分析和处理的职业技能。 —金旭亮老师 1.引入问题：寻路下山依旧回到我们假设的那个情境，如果我们在山顶，要以最快的方式下山，我们会怎么做？我们可能会环顾四周，然后找到最陡的方向，迈一小步，然后再找当前位置最陡的下山方向，再迈一小步… 暂时不用担心不同的起始点可能走到不同的局部最优点，因为事实证明用于线性回归的代价函数总是一个凸函数的形式,也就是说我们的山路其实应该是这样的： 而这里提到的最陡的方向，其实对应的就是数学里『梯度』的概念，也就是说，其实我们无需『伸脚试探』周边的陡峭程度，而是通过计算损失函数的梯度，直接取得最陡的方向。 我们知道一元函数在某一点的导数描述了这个函数在这一点附近的变化率。导数的本质是通过极限的概念对函数进行局部的线性逼近。 而对于多元的情况，梯度是上面情况的一个扩展，只不过这时候的变量不再是一个，而是多个，同时我们计算得到的『梯度方向』也是一个多维的向量。大家都知道数学上计算一元函数『梯度/导数』的表达式如下： 对于多元的情况，我们需要求每个方向的『偏导数』，然后把它们合在一块组成梯度向量。 可能你跟我一样把大一的高数都忘光了吧哈哈，赶紧来回顾一下。 2.知识回顾：偏导数及其几何意义在xOy平面内，当动点由P(x0,y0)沿不同方向变化时，函数f(x,y)的变化快慢一般说来是不同的，因此就需要研究f(x,y)在(x0,y0)点处沿不同方向的变化率。 因为曲面上的每一点都有无穷多条切线，描述这种函数的导数相当困难。偏导数就是选择其中一条切线，并求出它的斜率。通常，最感兴趣的是垂直于y轴（平行于xOz平面）的切线，以及垂直于x轴（平行于yOz平面）的切线。 我们希望求出函数在点（1, 1, 3）的对x的偏导数，对应的切线与xOz平面平行。求偏导的办法是把其他变量视为常数，我们把变量y视为常数，这是y = 1时的图像: 通过求这个图中的切线斜率，得出函数在点（1, 1, 3）的对x的偏导为3，记为： 3.梯度详解 假如一个空间中的每一点的属性都可以以一个标量来代表的话，那么这个场就是一个标量场。 假如一个空间中的每一点的属性都可以以一个向量来代表的话，那么这个场就是一个向量场。 标量场中某一点上的梯度指向标量场增长最快的方向，梯度的长度是这个最大的变化率。 对梯度直观的解释：如果现在的纯量场用一座山来表示，纯量值越大的地方越高，反之则越低，经过梯度的运算以后，会在这座山的每一个点上都算出一个向量，这个向量会指向每个点最陡的那个方向，而向量的大小则代表了这个最陡的方向到底有多陡。 求梯度的方法很简单，对每一个自变量求偏导数，然后将其偏导数作为自变量方向的坐标即可。梯度的符号为∇，则函数f(x,y)的梯度为： 以函数f(x)为例，先选择一个初始点，计算该点的梯度，然后按照梯度的方向更新自变量。若第k次迭代值为x(k)，则 其中α称作步长或者学习率，表示自变量每次迭代变化的大小。 一直按照上式更新自变量，直到当函数值变化非常小（如3%以内）或者达到最大迭代次数时停止，此时认为自变量更新到函数的极小值点。 梯度下降法的简单应用例1.求f(x)=x^2的极小值 f(x)的梯度为 ∇f(x)=2x 步长设置为0.1，选取自变量从3开始，则计算过程如下 可以看到随着迭代次数的增加，该函数越来越接近极小值点(0,0)，依据该方法一定可以找到精度允许范围内的极小值点。 例2.求f(x,y)=(x−10)^2+(y−10)^2的极小值 梯度为∇f(x,y)=(2x−20,2y−20) 步长设置为0.1，选择初始点为(20,20) 迭代代码： if __name__ == &quot;__main__&quot;: x = 20 y = 20 f = (x-10)*(x-10)+(y-10)*(y-10) alpha = 0.1 count = 20 while (count &gt; 0): x = x - alpha * (2*x-20) y = y - alpha * (2*y-20) f = (x-10)*(x-10)+(y-10)*(y-10) count = count - 1 print x, y, f 可以看到，随着我不断增加count值，(x,y)越来越接近于(10,10) 图中的黑色曲线即为梯度下降法下降时的轨迹： 梯度下降法求的是极小值，而不是最小值。 步长的选取很关键，步长过长达不到极值点甚至会发散，步长太短导致收敛时间过长。 斯坦福的机器学习视频中建议按照[0.001,0.003,0.01,0.03,…]的顺序尝试设置步长，同时观察函数值选择收敛最快的步长。 步长也可以设置为非固定值，根据迭代的情况变化。 J函数极值问题至此，我们都对梯度下降法非常熟悉了，用下面的公式迭代寻找θ 假设我们只有一个训练样本(x,y)，展开等式右边的偏导为 得到 派生出更为通用的m个训练样本的算法 下图中椭圆表示J函数的等高线，蓝色曲线即为梯度下降法下降时的轨迹 这个算法实际上被称为批梯度下降法（Batch gradient descent），它是指梯度下降算法的每一次迭代都需要遍历整个训练集合，因为你需要基于你的m个训练样本进行求和。然而在实际应用中，我们可能有一个非常非常大的训练集合，如果m是一个百万级的数，意味着每次梯度下降你都需要对i从1到百万进行一个求和。计算量如此巨大，显然是不合适的。 所以如果你有一个很大的训练集合，那么你应该使用随机梯度下降法（也称为增量梯度下降法）： 从样本集D拿出D1，按D1学习，走一步，再拿出D2，然后按D2学习，走第二步，不断地调整权值，从而最终得到一组权值使得我们的程序能够对一个新的样本输入得到正确的或无限接近的结果。","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"机器学习（一）：线性回归","date":"2017-02-08T11:04:58.000Z","path":"2017/02/08/回归与梯度下降/","text":"基于Andrew Ng（吴恩达）的机器学习课程 1.引入问题：房价预测假设有一个房屋销售记录表 我们将这些数据点画出来：横轴表示房屋面积，纵轴表示价格 给你一组这样的数据集，能否预测房屋面积和房价之间的关系？ 我们可以用一条直线去尽量准的拟合这些数据，然后如果有新的输入过来，我们可以在将直线上这个点对应的值返回，即高中学的“线性回归（regression）”方法。 2.学习过程首先声明一些概念和常用的符号： x代表输入变量，也称为特征（feature），在本例中代表房屋面积； y代表输出变量，有时也称为目标变量（target），在本例中代表房价； （x,y）表示一个样本，（xi,yi）代表第i个样本； 输入数据称为训练集（training set），在本例中代表房屋销售记录表。 下面是一个典型的机器学习的过程，首先给出一个训练集合，提供给学习算法，之后我们的算法会生成一个输出函数h，这个函数有能力对没有见过的新数据给出一个新的估计。 3.线性回归通常情况下许多回归问题都有多个输入特征，例如如果除了知道房子的大小之外，我们还知道房子的卧室数量： 那么，我们的训练集就可以有第二个特征，我们用 X1，X2..Xi 去描述 feature 里面的分量，比如 X1代表房子的面积，X2代表卧室数量， 等等，我们可以做出一个估计函数： 我们最终的目的就是要找到合适的参数值θ，θ = (θ1,θ2,….θn)，令X0=1，我们可以将函数h表示为： n表示特征的数目 程序需要一个机制去评估 h 函数选择的 θ 是否比较好，我们用一个损失函数（loss function），描述 h 函数不好的程度，称这个函数为 J 函数： m表示样本的数目 J函数以估计值h(i)与真实值 y(i)差的平方和作为错误估计，前面乘的 1/2 是为了后面在求导的时候消掉这个系数。 在选定线性回归模型后，只需要确定参数θ，就可以将模型用来预测。然而θ需要在 J(θ) 最小的情况下才能确定，因此问题归结为求J函数极小值问题。 4.初识梯度下降如何调整 θ 以使得 J(θ)取得最小值，使用梯度下降法（gradient descent ）。 梯度下降法是按下面的流程进行的： 首先对θ赋初始值，这个值可以是随机的，也可以让θ是一个全零的向量。 改变θ的值，使得J(θ)按梯度下降的方向进行减少。 在如图的三维坐标系中，我们可以将梯度下降想象成一个下山问题：选取一个初始点，每迈出一步，我往下山最快的方向走一步，最后到达最低点，J达到最小值。 让我们尝试从不同的位置开始再进行一遍梯度下降，一个稍微偏右的初始点，并且再一次找一个最陡峭的方向前进，如图，最终得到了一个完全不同的局部最优解： 这里我们注意到了，梯度下降的结果有时会依赖于参数初始值，求得的有可能不是全局极小值，这与初始点的选取有关。","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]}]