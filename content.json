[{"title":"幸得辛酸引路","date":"2017-02-16T11:04:58.000Z","path":"2017/02/16/共勉/","text":"昨晚看到考研失败的吴师兄一篇长长的圈文： 过去的2016年我非常努力了，拼了命试图改变进入大学后的颓势。三跨，从一个理科生工科男凭一股兴趣与热情去考北大国际关系。可惜不能偷偷摸摸干成一件大事给亲朋好友带来喜讯。初试排第10，然而至少得前6才能进复试，没忍住默默流了几分钟的泪。。 可流完了又如何，2016年用了7个月时间，只凭兴趣，干了别人整个本科的内容，刷了不下七八十本书，啃了无数论文。走出考场发现20分的课本死知识我没记住，答的完全不沾边，仿佛锥子扎心，提心吊胆，祈祷了一月多，现在也总算可以放下了，确实，差了口气，差了10分。实力不足，可又能怎么办，要看的内容浩如烟海，我怎么可能会认为那个点会考因而去死记住它。 也许是注定无缘吧，觉得到现在也该真正放下，去发掘其他路途上的精彩，而不要再执迷于一条道路了，一年又一年地耗了，毕竟我也不年轻了。大学就这样了，该开启找工作的生活了，再祈祷一次，希望这回运气多些，再多些，真的！希望过了今晚，明天阳光依旧明媚，希望我能多发现身边的美，而不再一直画着未来的大饼诱惑自己、执迷下去。 他是我大一寒假回百中宣传中大认识的师兄，一直非常佩服他。中大计算机跨考北大国际关系，难度可想而知。 考研路始终不是一条坦途，我不能把它想的太简单了。 引他一年前的话鞭策自己： 也许，也不需要去想那么多了。就纯粹为了精气神的逆转也行。至少假以时日回想起来，我不会因为自己居然全程颓废而痛苦不已。 要拼搏，也可以只是为了证明，我没有轻轻易易就彻底屈服了。最初的梦想，紧握在手上。最想去的地方，怎么能在半路就返航。 共勉。","tags":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/随笔/"}]},{"title":"机器学习（三）：最小二乘法","date":"2017-02-14T09:53:58.000Z","path":"2017/02/14/最小二乘法/","text":"基于Andrew Ng（吴恩达）的机器学习课程","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"机器学习（二）：梯度下降法","date":"2017-02-12T07:07:58.000Z","path":"2017/02/12/梯度下降的数学推导/","text":"基于Andrew Ng（吴恩达）的机器学习课程 在可以预期的未来，大数据与人工智能方面的工作机会将会快速增长，聪明的人已经在提前进行布局。要学习这个领域，我个人认为其难点不在于计算机技术本身，而在于相关的数学基础，以及对特定应用领域的了解。数学好，你才能够真正理解和用好各种算法，并进而掌握各种计算机工具、框架和平台，具备对数据进行各种分析和处理的职业技能。 —金旭亮老师 1.引入问题：寻路下山依旧回到我们假设的那个情境，如果我们在山顶，要以最快的方式下山，我们会怎么做？我们可能会环顾四周，然后找到最陡的方向，迈一小步，然后再找当前位置最陡的下山方向，再迈一小步… 暂时不用担心不同的起始点可能走到不同的局部最优点，因为事实证明用于线性回归的代价函数总是一个凸函数的形式,也就是说我们的山路其实应该是这样的： 而这里提到的最陡的方向，其实对应的就是数学里『梯度』的概念，也就是说，其实我们无需『伸脚试探』周边的陡峭程度，而是通过计算损失函数的梯度，直接取得最陡的方向。 我们知道一元函数在某一点的导数描述了这个函数在这一点附近的变化率。导数的本质是通过极限的概念对函数进行局部的线性逼近。 而对于多元的情况，梯度是上面情况的一个扩展，只不过这时候的变量不再是一个，而是多个，同时我们计算得到的『梯度方向』也是一个多维的向量。大家都知道数学上计算一元函数『梯度/导数』的表达式如下： 对于多元的情况，我们需要求每个方向的『偏导数』，然后把它们合在一块组成梯度向量。 可能你跟我一样把大一的高数都忘光了吧哈哈，赶紧来回顾一下。 2.知识回顾：偏导数及其几何意义在xOy平面内，当动点由P(x0,y0)沿不同方向变化时，函数f(x,y)的变化快慢一般说来是不同的，因此就需要研究f(x,y)在(x0,y0)点处沿不同方向的变化率。 因为曲面上的每一点都有无穷多条切线，描述这种函数的导数相当困难。偏导数就是选择其中一条切线，并求出它的斜率。通常，最感兴趣的是垂直于y轴（平行于xOz平面）的切线，以及垂直于x轴（平行于yOz平面）的切线。 我们希望求出函数在点（1, 1, 3）的对x的偏导数，对应的切线与xOz平面平行。求偏导的办法是把其他变量视为常数，我们把变量y视为常数，这是y = 1时的图像: 通过求这个图中的切线斜率，得出函数在点（1, 1, 3）的对x的偏导为3，记为： 3.梯度详解 假如一个空间中的每一点的属性都可以以一个标量来代表的话，那么这个场就是一个标量场。 假如一个空间中的每一点的属性都可以以一个向量来代表的话，那么这个场就是一个向量场。 标量场中某一点上的梯度指向标量场增长最快的方向，梯度的长度是这个最大的变化率。 对梯度直观的解释：如果现在的纯量场用一座山来表示，纯量值越大的地方越高，反之则越低，经过梯度的运算以后，会在这座山的每一个点上都算出一个向量，这个向量会指向每个点最陡的那个方向，而向量的大小则代表了这个最陡的方向到底有多陡。 求梯度的方法很简单，对每一个自变量求偏导数，然后将其偏导数作为自变量方向的坐标即可。梯度的符号为∇，则函数f(x,y)的梯度为： 以函数f(x)为例，先选择一个初始点，计算该点的梯度，然后按照梯度的方向更新自变量。若第k次迭代值为x(k)，则 其中α称作步长或者学习率，表示自变量每次迭代变化的大小。 一直按照上式更新自变量，直到当函数值变化非常小（如3%以内）或者达到最大迭代次数时停止，此时认为自变量更新到函数的极小值点。 梯度下降法的简单应用例1.求f(x)=x^2的极小值 f(x)的梯度为 ∇f(x)=2x 步长设置为0.1，选取自变量从3开始，则计算过程如下 可以看到随着迭代次数的增加，该函数越来越接近极小值点(0,0)，依据该方法一定可以找到精度允许范围内的极小值点。 例2.求f(x,y)=(x−10)^2+(y−10)^2的极小值 梯度为∇f(x,y)=(2x−20,2y−20) 步长设置为0.1，选择初始点为(20,20) 迭代代码： if __name__ == &quot;__main__&quot;: x = 20 y = 20 f = (x-10)*(x-10)+(y-10)*(y-10) alpha = 0.1 count = 20 while (count &gt; 0): x = x - alpha * (2*x-20) y = y - alpha * (2*y-20) f = (x-10)*(x-10)+(y-10)*(y-10) count = count - 1 print x, y, f 可以看到，随着我不断增加count值，(x,y)越来越接近于(10,10) 图中的黑色曲线即为梯度下降法下降时的轨迹： 梯度下降法求的是极小值，而不是最小值。 步长的选取很关键，步长过长达不到极值点甚至会发散，步长太短导致收敛时间过长。 斯坦福的机器学习视频中建议按照[0.001,0.003,0.01,0.03,…]的顺序尝试设置步长，同时观察函数值选择收敛最快的步长。 步长也可以设置为非固定值，根据迭代的情况变化。 J函数极值问题至此，我们都对梯度下降法非常熟悉了，用下面的公式迭代寻找θ 假设我们只有一个训练样本(x,y)，展开等式右边的偏导为 得到 派生出更为通用的m个训练样本的算法 下图中椭圆表示J函数的等高线，蓝色曲线即为梯度下降法下降时的轨迹 这个算法实际上被称为批梯度下降法（Batch gradient descent），它是指梯度下降算法的每一次迭代都需要遍历整个训练集合，因为你需要基于你的m个训练样本进行求和。然而在实际应用中，我们可能有一个非常非常大的训练集合，如果m是一个百万级的数，意味着每次梯度下降你都需要对i从1到百万进行一个求和。计算量如此巨大，显然是不合适的。 所以如果你有一个很大的训练集合，那么你应该使用随机梯度下降法（也称为增量梯度下降法）： 从样本集D拿出D1，按D1学习，走一步，再拿出D2，然后按D2学习，走第二步，不断地调整权值，从而最终得到一组权值使得我们的程序能够对一个新的样本输入得到正确的或无限接近的结果。","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"机器学习（一）：线性回归","date":"2017-02-08T11:04:58.000Z","path":"2017/02/08/回归与梯度下降/","text":"基于Andrew Ng（吴恩达）的机器学习课程 1.引入问题：房价预测假设有一个房屋销售记录表 我们将这些数据点画出来：横轴表示房屋面积，纵轴表示价格 给你一组这样的数据集，能否预测房屋面积和房价之间的关系？ 我们可以用一条直线去尽量准的拟合这些数据，然后如果有新的输入过来，我们可以在将直线上这个点对应的值返回，即高中学的“线性回归（regression）”方法。 2.学习过程首先声明一些概念和常用的符号： x代表输入变量，也称为特征（feature），在本例中代表房屋面积； y代表输出变量，有时也称为目标变量（target），在本例中代表房价； （x,y）表示一个样本，（xi,yi）代表第i个样本； 输入数据称为训练集（training set），在本例中代表房屋销售记录表。 下面是一个典型的机器学习的过程，首先给出一个训练集合，提供给学习算法，之后我们的算法会生成一个输出函数h，这个函数有能力对没有见过的新数据给出一个新的估计。 3.线性回归通常情况下许多回归问题都有多个输入特征，例如如果除了知道房子的大小之外，我们还知道房子的卧室数量： 那么，我们的训练集就可以有第二个特征，我们用 X1，X2..Xi 去描述 feature 里面的分量，比如 X1代表房子的面积，X2代表卧室数量， 等等，我们可以做出一个估计函数： 我们最终的目的就是要找到合适的参数值θ，θ = (θ1,θ2,….θn)，令X0=1，我们可以将函数h表示为： n表示特征的数目 程序需要一个机制去评估 h 函数选择的 θ 是否比较好，我们用一个损失函数（loss function），描述 h 函数不好的程度，称这个函数为 J 函数： m表示样本的数目 J函数以估计值h(i)与真实值 y(i)差的平方和作为错误估计，前面乘的 1/2 是为了后面在求导的时候消掉这个系数。 在选定线性回归模型后，只需要确定参数θ，就可以将模型用来预测。然而θ需要在 J(θ) 最小的情况下才能确定，因此问题归结为求J函数极小值问题。 4.初识梯度下降如何调整 θ 以使得 J(θ)取得最小值，使用梯度下降法（gradient descent ）。 梯度下降法是按下面的流程进行的： 首先对θ赋初始值，这个值可以是随机的，也可以让θ是一个全零的向量。 改变θ的值，使得J(θ)按梯度下降的方向进行减少。 在如图的三维坐标系中，我们可以将梯度下降想象成一个下山问题：选取一个初始点，每迈出一步，我往下山最快的方向走一步，最后到达最低点，J达到最小值。 让我们尝试从不同的位置开始再进行一遍梯度下降，一个稍微偏右的初始点，并且再一次找一个最陡峭的方向前进，如图，最终得到了一个完全不同的局部最优解： 这里我们注意到了，梯度下降的结果有时会依赖于参数初始值，求得的有可能不是全局极小值，这与初始点的选取有关。","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]}]