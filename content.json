[{"title":"机器学习（二）：梯度下降的数学推导","date":"2017-02-12T07:07:58.000Z","path":"2017/02/12/梯度下降的数学推导/","text":"基于Andrew Ng（吴恩达）的机器学习课程 1.引入问题：寻路下山依旧回到我们假设的那个情境，如果我们在山顶，要以最快的方式下山，我们会怎么做？我们可能会环顾四周，然后找到最陡的方向，迈一小步，然后再找当前位置最陡的下山方向，再迈一小步… 暂时不用担心不同的点可能走到不同的局部最优点，因为事实证明用于线性回归的代价函数总是一个凹函数的形式,也就是说我们的山路其实应该是这样的： 而这里提到的最陡的方向，其实对应的就是数学里『梯度』的概念，也就是说，其实我们无需『伸脚试探』周边的陡峭程度，而是通过计算损失函数的梯度，直接取得最陡的方向。 我们知道一元函数在某一点的导数描述了这个函数在这一点附近的变化率。导数的本质是通过极限的概念对函数进行局部的线性逼近。 而对于多元的情况，梯度是上面情况的一个扩展，只不过这时候的变量不再是一个，而是多个，同时我们计算得到的『梯度方向』也是一个多维的向量。大家都知道数学上计算一元函数『梯度/导数』的表达式如下： 对于多元的情况，我们需要求每个方向的『偏导数』，然后把它们合在一块组成梯度向量。 可能你跟我一样把大一的高数都忘光了吧哈哈，赶紧回顾一下。 2.知识回顾：偏导数及梯度在xOy平面内，当动点由P(x0,y0)沿不同方向变化时，函数f(x,y)的变化快慢一般说来是不同的，因此就需要研究f(x,y)在(x0,y0)点处沿不同方向的变化率。 因为曲面上的每一点都有无穷多条切线，描述这种函数的导数相当困难。偏导数就是选择其中一条切线，并求出它的斜率。通常，最感兴趣的是垂直于y轴（平行于xOz平面）的切线，以及垂直于x轴（平行于yOz平面）的切线。 这是图中y = 1时的图像: 对于单一的训练样本，这里给出了更新规则： 称为LMS更新规则（least mean squares 最小均方差），其中，α称为学习率，是一个能自己设定的常数，通常很小，下面还会讲到。","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"机器学习（一）：回归与梯度下降","date":"2017-02-08T11:04:58.000Z","path":"2017/02/08/回归与梯度下降/","text":"基于Andrew Ng（吴恩达）的机器学习课程 1.引入问题：房价预测假设有一个房屋销售记录表 我们将这些数据点画出来：横轴表示房屋面积，纵轴表示价格 给你一组这样的数据集，能否预测房屋面积和房价之间的关系？ 我们可以用一条直线去尽量准的拟合这些数据，然后如果有新的输入过来，我们可以在将直线上这个点对应的值返回，即高中学的“线性回归（regression）”方法。 2.学习过程首先声明一些概念和常用的符号： x代表输入变量，也称为特征（feature），在本例中代表房屋面积； y代表输出变量，有时也称为目标变量（target），在本例中代表房价； （x,y）表示一个样本，（xi,yi）代表第i个样本； 输入数据称为训练集（training set），在本例中代表房屋销售记录表。 下面是一个典型的机器学习的过程，首先给出一个训练集合，提供给学习算法，之后我们的算法会生成一个输出函数h，这个函数有能力对没有见过的新数据给出一个新的估计。 3.线性回归通常情况下许多回归问题都有多个输入特征，例如如果除了知道房子的大小之外，我们还知道房子的卧室数量： 那么，我们的训练集就可以有第二个特征，我们用 X1，X2..Xi 去描述 feature 里面的分量，比如 X1代表房子的面积，X2代表卧室数量， 等等，我们可以做出一个估计函数： 我们最终的目的就是要找到合适的参数值θ，令X0=1，我们可以将函数h表示为： n表示特征的数目 程序需要一个机制去评估 h 函数选择的 θ 是否比较好，我们用一个损失函数（loss function），描述 h 函数不好的程度，称这个函数为 J 函数： J函数以估计值h(i)与真实值 y(i)差的平方和作为错误估计，前面乘的 1/2 是为了后面在求导的时候消掉这个系数。 在选定线性回归模型后，只需要确定参数θ，就可以将模型用来预测。然而θ需要在 J(θ) 最小的情况下才能确定，因此问题归结为求J函数极小值问题。 4.梯度下降法如何调整 θ 以使得 J(θ)取得最小值，使用梯度下降法（gradient descent ）。 梯度下降法是按下面的流程进行的： 首先对θ赋初始值，这个值可以是随机的，也可以让θ是一个全零的向量。 改变θ的值，使得J(θ)按梯度下降的方向进行减少。 在如图的三维坐标系中，我们可以将梯度下降想象成一个下山问题：选取一个初始点，每迈出一步，我往下山最快的方向走一步，最后到达最低点，J达到最小值。 让我们尝试从不同的位置开始再进行一遍梯度下降，一个稍微偏右的初始点，并且再一次找一个最陡峭的方向前进，如图，最终得到了一个完全不同的局部最优解： 这里我们注意到了，梯度下降的结果有时会依赖于参数初始值，求得的有可能不是全局极小值，这与初始点的选取有关。","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]}]