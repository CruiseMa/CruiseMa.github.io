[{"title":"机器学习（二）：梯度下降的数学推导","date":"2017-02-12T07:07:58.000Z","path":"2017/02/12/梯度下降的数学推导/","text":"基于Andrew Ng（吴恩达）的机器学习课程 1.引入问题：寻路下山依旧回到我们假设的那个情境，如果我们在山顶，要以最快的方式下山，我们会怎么做？我们可能会环顾四周，然后找到最陡的方向，迈一小步，然后再找当前位置最陡的下山方向，再迈一小步… 暂时不用担心不同的起始点可能走到不同的局部最优点，因为事实证明用于线性回归的代价函数总是一个凸函数的形式,也就是说我们的山路其实应该是这样的： 而这里提到的最陡的方向，其实对应的就是数学里『梯度』的概念，也就是说，其实我们无需『伸脚试探』周边的陡峭程度，而是通过计算损失函数的梯度，直接取得最陡的方向。 我们知道一元函数在某一点的导数描述了这个函数在这一点附近的变化率。导数的本质是通过极限的概念对函数进行局部的线性逼近。 而对于多元的情况，梯度是上面情况的一个扩展，只不过这时候的变量不再是一个，而是多个，同时我们计算得到的『梯度方向』也是一个多维的向量。大家都知道数学上计算一元函数『梯度/导数』的表达式如下： 对于多元的情况，我们需要求每个方向的『偏导数』，然后把它们合在一块组成梯度向量。 可能你跟我一样把大一的高数都忘光了吧哈哈，赶紧来回顾一下。 2.知识回顾：偏导数及其几何意义在xOy平面内，当动点由P(x0,y0)沿不同方向变化时，函数f(x,y)的变化快慢一般说来是不同的，因此就需要研究f(x,y)在(x0,y0)点处沿不同方向的变化率。 因为曲面上的每一点都有无穷多条切线，描述这种函数的导数相当困难。偏导数就是选择其中一条切线，并求出它的斜率。通常，最感兴趣的是垂直于y轴（平行于xOz平面）的切线，以及垂直于x轴（平行于yOz平面）的切线。 我们希望求出函数在点（1, 1, 3）的对x的偏导数，对应的切线与xOz平面平行。求偏导的办法是把其他变量视为常数，我们把变量y视为常数，这是y = 1时的图像: 通过求这个图中的切线斜率，得出函数在点（1, 1, 3）的对x的偏导为3，记为： 3.梯度详解 假如一个空间中的每一点的属性都可以以一个标量来代表的话，那么这个场就是一个标量场。 假如一个空间中的每一点的属性都可以以一个向量来代表的话，那么这个场就是一个向量场。 标量场中某一点上的梯度指向标量场增长最快的方向，梯度的长度是这个最大的变化率。 对梯度直观的解释：如果现在的纯量场用一座山来表示，纯量值越大的地方越高，反之则越低，经过梯度的运算以后，会在这座山的每一个点上都算出一个向量，这个向量会指向每个点最陡的那个方向，而向量的大小则代表了这个最陡的方向到底有多陡。 求梯度的方法很简单，对每一个自变量求偏导数，然后将其偏导数作为自变量方向的坐标即可。梯度的符号为∇，则函数f(x,y)的梯度为： 以函数f(x)为例，先选择一个初始点，计算该点的梯度，然后按照梯度的方向更新自变量。若第k次迭代值为x(k)，则 其中α称作步长或者学习率，表示自变量每次迭代变化的大小。 一直按照上式更新自变量，直到当函数值变化非常小（如3%以内）或者达到最大迭代次数时停止，此时认为自变量更新到函数的极小值点。 梯度下降法的简单应用例1.求f(x)=x^2的极小值 f(x)的梯度为 ∇f(x)=2x 步长设置为0.1，选取自变量从3开始，则计算过程如下 可以看到随着迭代次数的增加，该函数越来越接近极小值点(0,0)，依据该方法一定可以找到精度允许范围内的极小值点。 例2.求f(x,y)=(x−10)^2+(y−10)^2的极小值 梯度为∇f(x,y)=(2(x−10),2(y−10)) 步长设置为0.1，选择初始点为(20,20)，这次以图形表示计算过程，图中的黑色曲线即为梯度下降法下降时的轨迹，效果非常好。 斜视图： 俯视图： 梯度下降法求的是极小值，而不是最小值。 梯度下降法常常用来求凸函数的最小值，例如机器学习中各种代价函数的最小值。 步长的选取很关键，步长过长达不到极值点甚至会发散，步长太短导致收敛时间过长。 斯坦福的机器学习视频中建议按照[0.001,0.003,0.01,0.03,…]的顺序尝试设置步长，同时观察函数值选择收敛最快的步长。 步长也可以设置为非固定值，根据迭代的情况变化。 下降的初始点一般设置为从原点开始。 J函数极值问题梯度下降法用下面的公式迭代寻找θ 其中参数a称为learning rate（a&gt;0），a越大每一步下降的幅度越大速度也会越快，但过大有可能导致算法不准确。 假设我们只有一个训练样本(x,y)，展开等式右边的偏导为 得到 称为LMS更新规则（least mean squares 最小均方差） 证明","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"机器学习（一）：回归与梯度下降","date":"2017-02-08T11:04:58.000Z","path":"2017/02/08/回归与梯度下降/","text":"基于Andrew Ng（吴恩达）的机器学习课程 1.引入问题：房价预测假设有一个房屋销售记录表 我们将这些数据点画出来：横轴表示房屋面积，纵轴表示价格 给你一组这样的数据集，能否预测房屋面积和房价之间的关系？ 我们可以用一条直线去尽量准的拟合这些数据，然后如果有新的输入过来，我们可以在将直线上这个点对应的值返回，即高中学的“线性回归（regression）”方法。 2.学习过程首先声明一些概念和常用的符号： x代表输入变量，也称为特征（feature），在本例中代表房屋面积； y代表输出变量，有时也称为目标变量（target），在本例中代表房价； （x,y）表示一个样本，（xi,yi）代表第i个样本； 输入数据称为训练集（training set），在本例中代表房屋销售记录表。 下面是一个典型的机器学习的过程，首先给出一个训练集合，提供给学习算法，之后我们的算法会生成一个输出函数h，这个函数有能力对没有见过的新数据给出一个新的估计。 3.线性回归通常情况下许多回归问题都有多个输入特征，例如如果除了知道房子的大小之外，我们还知道房子的卧室数量： 那么，我们的训练集就可以有第二个特征，我们用 X1，X2..Xi 去描述 feature 里面的分量，比如 X1代表房子的面积，X2代表卧室数量， 等等，我们可以做出一个估计函数： 我们最终的目的就是要找到合适的参数值θ，θ = (θ1,θ2,….θn)，令X0=1，我们可以将函数h表示为： n表示特征的数目 程序需要一个机制去评估 h 函数选择的 θ 是否比较好，我们用一个损失函数（loss function），描述 h 函数不好的程度，称这个函数为 J 函数： m表示样本的数目 J函数以估计值h(i)与真实值 y(i)差的平方和作为错误估计，前面乘的 1/2 是为了后面在求导的时候消掉这个系数。 在选定线性回归模型后，只需要确定参数θ，就可以将模型用来预测。然而θ需要在 J(θ) 最小的情况下才能确定，因此问题归结为求J函数极小值问题。 4.梯度下降法如何调整 θ 以使得 J(θ)取得最小值，使用梯度下降法（gradient descent ）。 梯度下降法是按下面的流程进行的： 首先对θ赋初始值，这个值可以是随机的，也可以让θ是一个全零的向量。 改变θ的值，使得J(θ)按梯度下降的方向进行减少。 在如图的三维坐标系中，我们可以将梯度下降想象成一个下山问题：选取一个初始点，每迈出一步，我往下山最快的方向走一步，最后到达最低点，J达到最小值。 让我们尝试从不同的位置开始再进行一遍梯度下降，一个稍微偏右的初始点，并且再一次找一个最陡峭的方向前进，如图，最终得到了一个完全不同的局部最优解： 这里我们注意到了，梯度下降的结果有时会依赖于参数初始值，求得的有可能不是全局极小值，这与初始点的选取有关。","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]}]